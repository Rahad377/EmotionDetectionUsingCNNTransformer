{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nimport librosa\nimport librosa.display\nimport IPython.display as ipd\nimport matplotlib.pyplot as plt \nimport torch\nimport torch.nn as nn\nimport torchaudio","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class DeepFeatures2(nn.Module):\n    def __init__(self):\n        super(DeepFeatures2,self).__init__()\n        self.conv1 = nn.Sequential(\n                    nn.Conv1d(in_channels=40,out_channels=64,kernel_size=3,stride=1),\n                    nn.BatchNorm1d(64),\n                    nn.ReLU(),\n                    nn.Dropout1d(0.2),\n                    nn.Conv1d(in_channels=64,out_channels=64,kernel_size=3,stride=1),\n                    nn.BatchNorm1d(64),\n                    nn.ReLU(),\n                    nn.MaxPool1d(kernel_size=3),\n                    nn.Dropout1d(0.2)\n                    )\n        self.conv2 = nn.Sequential(\n                    nn.Conv1d(in_channels=64,out_channels=128,kernel_size=3,stride=1),\n                    nn.BatchNorm1d(128),\n                    nn.ReLU(),\n                    nn.Conv1d(in_channels=128,out_channels=64,kernel_size=3,stride=1),\n                    nn.BatchNorm1d(64),\n                    nn.ReLU(),\n                    nn.MaxPool1d(kernel_size=3)\n                    )\n        \n        #encoder_layers = TransformerEncoderLayer(d_model=64, nhead=4, dim_feedforward=200,batch_first=True)\n        #self.encoder = TransformerEncoder(encoder_layers, 2)\n        self.attn1 = nn.MultiheadAttention(embed_dim=64, num_heads=4, batch_first=True)\n        self.ln1 = nn.LayerNorm([31,64])\n        self.mlp1 = nn.Linear(64,64)\n        self.ln_mlp1 = nn.LayerNorm([31,64])\n        \n        self.attn2 = nn.MultiheadAttention(embed_dim=64, num_heads=4, batch_first=True)\n        self.ln2 = nn.LayerNorm([31,64])\n        self.mlp2 = nn.Linear(64,64)\n        self.ln_mlp2 = nn.LayerNorm([31,64])\n        \n        self.linear = nn.Sequential(\n                    nn.Linear(31*128,512),\n                    nn.ReLU(),\n                    nn.Dropout(0.3),\n                    nn.Linear(512,5)\n                    )\n    \n    def forward(self,x):\n        x = x.squeeze(1)\n        x = self.conv1(x)\n        #print(x.shape)\n        x = self.conv2(x)\n        \n        \n        #print(x.shape)\n        x = x.permute(0,2,1) # N,31,64 \n        z = x.clone()\n        \n        #print(x.shape)\n        y,_ = self.attn1(x,x,x)\n        x= y.clamp(min=0) + x\n        x = self.ln1(x) # N,31,64\n        \n        x=self.mlp1(x).clamp(min=0) + x\n        x=self.ln_mlp1(x)\n        \n        y,_ = self.attn2(x,x,x)\n        x= y.clamp(min=0) + x\n        x = self.ln_mlp2(x) # N,31,64\n        \n        x=self.mlp2(x).clamp(min=0) + x\n        x=self.ln2(x)\n        \n        x = torch.cat((z,x),dim=2)\n        #print(x.shape)\n        x = x.view(-1,31*128)\n        #print(x.shape)\n        x = self.linear(x)\n        #print(x.shape)\n        return x","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}